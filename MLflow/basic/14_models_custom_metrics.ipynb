{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage format \n",
    "\n",
    "- specific how the model is packaged and saved\n",
    "- include the model, metadata of the model, hypreparameters and the model's version\n",
    "- supports multiple storage formats - a directory of files, a single file format, python functions or container images\n",
    "\n",
    "## Model signature\n",
    "\n",
    "- specific the input, output data types and shapes that the model expects and returns.\n",
    "- used by MLflow to generate REST API for the model\n",
    "- defined using the python function annotations syntax\n",
    "- store as part of the MLflow model and can be accessed by other MLflow components.\n",
    "\n",
    "## Model API\n",
    "\n",
    "- A REST API providing a standardized interface for interacting with model\n",
    "- API supports both synchronous and asynchronous requests\n",
    "- Can be used for real time inference or batch processing\n",
    "- can be deployed to various environments cloud platforms, edge devices or on-premises servers\n",
    "\n",
    "## Flavor\n",
    "\n",
    "- refer to a specific way of serializing and storing a machine learning model\n",
    "- each of the supported frameworks and libraries has an associated flavor in MLflow\n",
    "- additional community-driven flavors and custom flavors\n",
    "\n",
    "## Miscalleneous\n",
    "\n",
    "- Provides functionality for evaluatingthe model using metrics such as accuracy, precision, recall, F1 score\n",
    "- Provides tools to deploy MLflow models to various platforms\n",
    "- Can set a custom target by specifying a custom deployment target along with the necessary code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model API\n",
    "\n",
    "- save model\n",
    "- log model\n",
    "- load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pathlib import Path\n",
    "import os\n",
    "from mlflow.models.signature import ModelSignature, infer_signature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "import sklearn\n",
    "import joblib\n",
    "import cloudpickle\n",
    "from mlflow.models import make_metric\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process log tracking\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    filename='./logfile.log',\n",
    "                    filemode='w', # 'w' 表示寫模式, 'a' 表示追加模式, 'w' 表示如果文件已存在，先将其清空。如果你想在不清空现有日志的情况下向文件追加日志，可以使用 'a' 模式。\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## raw data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(40)\n",
    "\n",
    "# Read the wine-quality csv file from the URL\n",
    "data = pd.read_csv(\"data/red-wine-quality.csv\")\n",
    "# os.mkdir(\"data/\")\n",
    "data.to_csv(\"data/red-wine-quality.csv\", index=False)\n",
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "data_dir = 'red-wine-data'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "data.to_csv(data_dir + '/data.csv')\n",
    "train.to_csv(data_dir + '/train.csv')\n",
    "test.to_csv(data_dir + '/test.csv')\n",
    "\n",
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "train_x = train.drop([\"quality\"], axis=1)\n",
    "test_x = test.drop([\"quality\"], axis=1)\n",
    "train_y = train[[\"quality\"]]\n",
    "test_y = test[[\"quality\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "l1_ratio = 0.9  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tracking uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The set tracking uri is  \n"
     ]
    }
   ],
   "source": [
    "# set tracking folder\n",
    "mlflow.set_tracking_uri(uri=\"\")\n",
    "\n",
    "# 全路徑寫法 file:xxxx\n",
    "# mlflow.set_tracking_uri(uri=r\"file:C:\\Users\\xdxd2\\Sunny_VS_worksapce\\Sunny_python\\ML\\mytracks\")\n",
    "\n",
    "print(\"The set tracking uri is \", mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/31 12:28:20 INFO mlflow.tracking.fluent: Experiment with name 'experiment_custom_metrics' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: experiment_custom_metrics\n",
      "Experiment_id: 370280449364329886\n",
      "Artifact Location: file:///C:/Users/xdxd2/Sunny_VS_worksapce/Sunny_python/ML/MLOps_fundamentals/MLflow/basic/mlruns/370280449364329886\n",
      "Tags: {}\n",
      "Lifecycle_stage: active\n",
      "Creation timestamp: 1706675300388\n"
     ]
    }
   ],
   "source": [
    "exp = mlflow.set_experiment(experiment_name=\"experiment_custom_metrics\")\n",
    "\n",
    "print(\"Name: {}\".format(exp.name))\n",
    "print(\"Experiment_id: {}\".format(exp.experiment_id))\n",
    "print(\"Artifact Location: {}\".format(exp.artifact_location))\n",
    "print(\"Tags: {}\".format(exp.tags))\n",
    "print(\"Lifecycle_stage: {}\".format(exp.lifecycle_stage))\n",
    "print(\"Creation timestamp: {}\".format(exp.creation_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticnet model (alpha=0.900000, l1_ratio=0.900000):\n",
      "  RMSE: 0.8312296853893981\n",
      "  MAE: 0.6673520215793272\n",
      "  R2: 0.02101549378688994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea69355e54a470780e9df2425422ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8575908256d477987fbfdc101bcbb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/31 12:28:24 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2024/01/31 12:28:24 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "2024/01/31 12:28:24 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/01/31 12:28:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: squared diff plus one\n",
      "2024/01/31 12:28:24 INFO mlflow.models.evaluation.default_evaluator: Evaluating metrics: sum on target divided by two\n",
      "2024/01/31 12:28:27 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Permutation is used.\n",
      "Permutation explainer: 401it [00:20, 12.08it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The artifact path is file:///C:/Users/xdxd2/Sunny_VS_worksapce/Sunny_python/ML/MLOps_fundamentals/MLflow/basic/mlruns/370280449364329886/5ae123d3e78b4fa8a3155c7c594a22ad/artifacts\n",
      "Active run id is 5ae123d3e78b4fa8a3155c7c594a22ad\n",
      "Active run name is evaluate_model\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run(experiment_id=exp.experiment_id, run_name=\"evaluate_model\")\n",
    "\n",
    "tags = {\n",
    "    \"engineering\": \"ML platform\",\n",
    "    \"release.candidate\": \"RC1\",\n",
    "    \"release.version\": \"2.0\"\n",
    "}\n",
    "\n",
    "mlflow.set_tags(tags)\n",
    "mlflow.sklearn.autolog(\n",
    "    log_input_examples=False,\n",
    "    log_model_signatures=False,\n",
    "    log_models=False\n",
    ")\n",
    "lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "lr.fit(train_x, train_y)\n",
    "\n",
    "predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "print(\"Elasticnet model (alpha={:f}, l1_ratio={:f}):\".format(\n",
    "    alpha, l1_ratio))\n",
    "print(\"  RMSE: %s\" % rmse)\n",
    "print(\"  MAE: %s\" % mae)\n",
    "print(\"  R2: %s\" % r2)\n",
    "\n",
    "mlflow.log_params({\n",
    "    \"alpha\": 0.9,\n",
    "    \"l1_ratio\": 0.9\n",
    "})\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"rmse\": rmse,\n",
    "    \"r2\": r2,\n",
    "    \"mae\": mae\n",
    "})\n",
    "\n",
    "sklearn_model_path = \"sklearn_model.pkl\"\n",
    "joblib.dump(lr, sklearn_model_path)\n",
    "artifacts = {\n",
    "    \"sklearn_model\": sklearn_model_path,\n",
    "    \"data\": data_dir\n",
    "}\n",
    "\n",
    "class SklearnWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.sklearn_model = joblib.load(\n",
    "            context.artifacts[\"sklearn_model\"])\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.sklearn_model.predict(model_input.values)\n",
    "\n",
    "# Create a Conda environment for the new MLflow Model that contains all necessary dependencies.\n",
    "conda_env = {\n",
    "    \"channels\": [\"defaults\"],\n",
    "    \"dependencies\": [\n",
    "        \"python={}\".format(3.10),\n",
    "        \"pip\",\n",
    "        {\n",
    "            \"pip\": [\n",
    "                \"mlflow=={}\".format(mlflow.__version__),\n",
    "                \"scikit-learn=={}\".format(sklearn.__version__),\n",
    "                \"cloudpickle=={}\".format(cloudpickle.__version__),\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"name\": \"sklearn_env\",\n",
    "}\n",
    "\n",
    "mlflow.pyfunc.log_model(\n",
    "    artifact_path=\"sklear_mlflow_pyfunc\",\n",
    "    python_model=SklearnWrapper(),\n",
    "    artifacts=artifacts,\n",
    "    code_path=[\"12_models_load_model.ipynb\"],\n",
    "    conda_env=conda_env\n",
    ")\n",
    "\n",
    "# _builtin_metrics -> 非內建指標\n",
    "# builtin_metrics -> 內建指標\n",
    "\n",
    "def squared_diff_plus_one(eval_df, _builtin_metrics):\n",
    "    return np.sum(np.abs(eval_df[\"prediction\"] - eval_df[\"target\"] + 1) ** 2)\n",
    "\n",
    "def sum_on_target_divided_by_two(_eval_df, builtin_metrics):\n",
    "    return builtin_metrics[\"sum_on_target\"] / 2\n",
    "\n",
    "squared_diff_plus_one_metric = make_metric(\n",
    "    eval_fn=squared_diff_plus_one,\n",
    "    greater_is_better=False,\n",
    "    name=\"squared diff plus one\"\n",
    ")\n",
    "\n",
    "sum_on_target_divided_by_two_metric = make_metric(\n",
    "    eval_fn=sum_on_target_divided_by_two,\n",
    "    greater_is_better=True,\n",
    "    name=\"sum on target divided by two\"\n",
    ")\n",
    "\n",
    "\n",
    "def prediction_target_scatter(eval_df, _builtin_metrics, artifacts_dir):\n",
    "    plt.scatter(eval_df[\"prediction\"], eval_df[\"target\"])\n",
    "    plt.xlabel(\"Targets\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(\"Targets vs. Predictions\")\n",
    "    plot_path = os.path.join(artifacts_dir, \"example_scatter_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    return {\"example_scatter_plot_artifact\": plot_path}\n",
    "\n",
    "\n",
    "artifacts_uri = mlflow.get_artifact_uri(\"sklear_mlflow_pyfunc\")\n",
    "mlflow.evaluate(\n",
    "    artifacts_uri,\n",
    "    test,\n",
    "    targets=\"quality\",\n",
    "    model_type=\"regressor\",\n",
    "    evaluators=[\"default\"],\n",
    "    custom_metrics=[\n",
    "        squared_diff_plus_one_metric,\n",
    "        sum_on_target_divided_by_two_metric\n",
    "    ],\n",
    "    custom_artifacts=[prediction_target_scatter]\n",
    ")\n",
    "\n",
    "\n",
    "artifacts_uri = mlflow.get_artifact_uri()\n",
    "print(\"The artifact path is\", artifacts_uri)\n",
    "mlflow.end_run()\n",
    "\n",
    "run = mlflow.last_active_run()\n",
    "print(\"Active run id is {}\".format(run.info.run_id))\n",
    "print(\"Active run name is {}\".format(run.info.run_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RMSE for test data: 0.7442929001520973\n",
      "  MAE for test data: 0.5763000946156918\n",
      "  R2 for test data: 0.21508707276848893\n"
     ]
    }
   ],
   "source": [
    "mlflow.start_run()\n",
    "\n",
    "id = mlflow.pyfunc.load_model(model_uri=\"runs:/cb2d193e28e54b5c845a56792941583c/sklear_mlflow_pyfunc\")\n",
    "predicted_qualities = id.predict(test_x)\n",
    "(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "print(\"  RMSE for test data: %s\" % rmse)\n",
    "print(\"  MAE for test data: %s\" % mae)\n",
    "print(\"  R2 for test data: %s\" % r2)\n",
    "\n",
    "mlflow.end_run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
